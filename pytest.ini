# NIS TOOLKIT SUIT v3.2.1 - Pytest Configuration

[tool:pytest]
# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test directories
testpaths = 
    tests
    testing
    nis-core-toolkit/tests
    nis-agent-toolkit/tests
    nis-integrity-toolkit/tests

# Minimum Python version
minversion = 6.0

# Add options
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --color=yes
    --durations=10
    --cov-report=term-missing
    --cov-report=html:testing/reports/coverage_html
    --cov-report=xml:testing/reports/coverage.xml
    --cov-report=json:testing/reports/coverage.json
    --junit-xml=testing/reports/junit.xml
    --json-report
    --json-report-file=testing/reports/pytest.json

# Coverage configuration
# Note: These can be overridden by pyproject.toml or .coveragerc
[coverage:run]
source = 
    nis-core-toolkit/src
    nis-agent-toolkit/core
    nis-integrity-toolkit
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*
    */.pytest_cache/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

# Test markers
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    performance: Performance tests
    security: Security tests
    slow: Tests that take a long time to run
    quick: Fast tests for development
    benchmark: Performance benchmark tests
    regression: Regression tests
    smoke: Smoke tests for basic functionality
    api: API tests
    database: Database tests
    network: Network tests
    docker: Tests requiring Docker
    kubernetes: Tests requiring Kubernetes
    gpu: Tests requiring GPU
    memory_intensive: Tests that use significant memory
    cpu_intensive: Tests that use significant CPU
    flaky: Tests that may fail intermittently
    skip_ci: Tests to skip in CI environment
    requires_internet: Tests requiring internet connection
    requires_auth: Tests requiring authentication
    xfail: Tests expected to fail
    parametrize: Parametrized tests

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = testing/reports/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Asyncio configuration
asyncio_mode = auto

# Warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    error::UserWarning

# Custom markers for test collection
empty_parameter_set_mark = xfail

# Timeout for tests (in seconds)
timeout = 300

# Parallel execution
# Uncomment to enable parallel test execution
# -n auto

# Test order
# Uncomment to randomize test order
# --random-order

# Benchmark configuration (for pytest-benchmark)
benchmark_max_time = 5.0
benchmark_min_rounds = 3
benchmark_warmup = true
benchmark_warmup_iterations = 3
benchmark_disable_gc = true
benchmark_sort = mean
